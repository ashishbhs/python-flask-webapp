import json
from sklearn.metrics import precision_score, recall_score, f1_score

def load_json(file_path):
    with open(file_path, 'r') as f:
        return json.load(f)

def compare_json(ground_truth, llm_output):
    gt_keys = ground_truth.keys()
    total_keys = len(gt_keys)

    matched = 0
    y_true = []
    y_pred = []

    mismatches = {}
    missing_keys = []

    for key in gt_keys:
        gt_val = str(ground_truth.get(key)).strip().lower()
        llm_val = str(llm_output.get(key, "")).strip().lower()

        y_true.append(1)  # Relevant key exists in ground truth
        if llm_val == gt_val and key in llm_output:
            y_pred.append(1)
            matched += 1
        else:
            y_pred.append(0)
            mismatches[key] = {"expected": ground_truth.get(key), "got": llm_output.get(key, None)}
            if key not in llm_output:
                missing_keys.append(key)

    accuracy = matched / total_keys * 100
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return {
        "Total Keys": total_keys,
        "Exact Matches": matched,
        "Accuracy (%)": round(accuracy, 2),
        "Precision": round(precision, 2),
        "Recall": round(recall, 2),
        "F1 Score": round(f1, 2),
        "Missing Keys": missing_keys,
        "Mismatches": mismatches
    }

# === Usage ===
ground_truth_file = "ground_truth.json"
llm_output_file = "llm_output.json"

ground_truth = load_json(ground_truth_file)
llm_output = load_json(llm_output_file)

results = compare_json(ground_truth, llm_output)

# Display results
for key, val in results.items():
    print(f"{key}: {val}")


pip install scikit-learn fuzzywuzzy python-Levenshtein

import json
from sklearn.metrics import precision_score, recall_score, f1_score
from fuzzywuzzy import fuzz

# --- Helper to flatten nested JSON ---
def flatten_json(y, prefix=''):
    out = {}
    def flatten(x, name=''):
        if isinstance(x, dict):
            for a in x:
                flatten(x[a], name + a + '.')
        elif isinstance(x, list):
            for i, a in enumerate(x):
                flatten(a, name + str(i) + '.')
        else:
            out[name[:-1]] = str(x).strip().lower()
    flatten(y, prefix)
    return out

# --- Comparison Function ---
def compare_json(llm_json_path, ground_truth_path, fuzzy_threshold=90):
    # Load JSON files
    with open(llm_json_path, 'r') as f1, open(ground_truth_path, 'r') as f2:
        llm_data = json.load(f1)
        ground_truth = json.load(f2)

    # Flatten both JSONs
    llm_flat = flatten_json(llm_data)
    gt_flat = flatten_json(ground_truth)

    # Union of keys to cover all fields
    all_keys = set(llm_flat.keys()) | set(gt_flat.keys())

    total = len(all_keys)
    correct = 0
    y_true = []
    y_pred = []
    detailed_report = {}

    for key in all_keys:
        gt_value = gt_flat.get(key, "")
        llm_value = llm_flat.get(key, "")
        
        if not gt_value and not llm_value:
            match = True
        elif gt_value == llm_value:
            match = True
        else:
            similarity = fuzz.ratio(gt_value, llm_value)
            match = similarity >= fuzzy_threshold
        
        y_true.append(1)
        y_pred.append(1 if match else 0)

        if match:
            correct += 1

        detailed_report[key] = {
            "ground_truth": gt_value,
            "llm_output": llm_value,
            "match": match,
            "similarity": fuzz.ratio(gt_value, llm_value)
        }

    # Metrics
    accuracy = correct / total if total else 0.0
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    # Output
    print("=== JSON Comparison Metrics ===")
    print(f"Total fields compared: {total}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1:.2f}")

    print("\n=== Detailed Comparison ===")
    for key, report in detailed_report.items():
        print(f"{key} â†’ Match: {report['match']}, Similarity: {report['similarity']}%")
        print(f"   GT:  {report['ground_truth']}")
        print(f"   LLM: {report['llm_output']}\n")

# --- Example usage ---
# compare_json("llm_output.json", "ground_truth.json", fuzzy_threshold=85)
